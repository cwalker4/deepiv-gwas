{
    "stage": "treatment",
    "hidden_layers": [
        200,
        200,
        200
    ],
    "activations": "relu",
    "output_activation": "relu",
    "constrain_norm": 0,
    "l2": 0,
    "dropout_rate": 0,
    "learning_rate": 0.001,
    "batch_size": 100,
    "num_epochs": 100,
    "loss_fn": "mse",
    "learning_rates": 0.0001
}