{

    "stage":  "treatment",

    "hidden_layers": [200, 200],
    "activations": "relu",
    "output_activation": "relu",

    "constrain_norm": 0,
    "l2": 0.0001,
    "dropout_rate": 0,

    "learning_rate": 1e-3,
    "batch_size": 100,
    "num_epochs": 100,
    "loss_fn": "mse"

}
