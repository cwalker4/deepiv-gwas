{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "from deepiv.models import Treatment, Response\n",
    "import deepiv.architectures as architectures\n",
    "import deepiv.densities as densities\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_levels_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/expression_levels_subset.csv')\n",
    "gene_variants_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/gene_variants.csv')\n",
    "outcomes_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim(z) =  (13980, 12667)\n",
      "Dim(p) =  (2344, 12667)\n",
      "Dim(y) =  (12666, 2)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "assert(gene_variants_raw.shape[1] - 1 == outcomes_raw.shape[0])\n",
    "print(\"Dim(z) = \", gene_variants_raw.shape)\n",
    "print(\"Dim(p) = \", expression_levels_raw.shape)\n",
    "print(\"Dim(y) = \", outcomes_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_levels = expression_levels_raw.drop('mrna', axis = 1).transpose().as_matrix()\n",
    "gene_variants = gene_variants_raw.drop('hugo', axis = 1).transpose().as_matrix()\n",
    "outcomes = outcomes_raw['outcome'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim(z) =  (12666, 13980)\n",
      "Dim(p) =  (12666, 2344)\n",
      "Dim(y) =  (12666,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dim(z) = \", gene_variants.shape)\n",
    "print(\"Dim(p) = \", expression_levels.shape)\n",
    "print(\"Dim(y) = \", outcomes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train, z_test, p_train, p_test, y_train, y_test = train_test_split(gene_variants, expression_levels, outcomes, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training first stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting treatment\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1156: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/118\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 2344) for Tensor 'concatenate_1_target:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-34c8f427c076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m treatment_model.fit(gene_variants, expression_levels,\n\u001b[1;32m     25\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                    batch_size = batch_size)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1105\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100, 2344) for Tensor 'concatenate_1_target:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "instruments = Input(shape=(gene_variants.shape[1],), name = \"instruments\")\n",
    "hidden = [7000, 4000, 3000]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "est_treat = architectures.feed_forward_net(instruments,\n",
    "                                          lambda x: densities.mixture_of_gaussian_output(x, n_components),\n",
    "                                          hidden_layers = hidden,\n",
    "                                          activations = activation)\n",
    "\n",
    "treatment_model = Treatment(inputs=[instruments], outputs=est_treat)\n",
    "print(\"Fitting treatment\")\n",
    "treatment_model.compile('adam',\n",
    "                       loss='mixture_of_gaussians',\n",
    "                       n_components = n_components)\n",
    "\n",
    "treatment_model.fit(gene_variants, expression_levels,\n",
    "                   epochs = epochs,\n",
    "                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 8s - loss: 1333658.1120     \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 7s - loss: 1283039.6250     \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 8s - loss: 1048776.2033     \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 8s - loss: 693894.6032     \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 9s - loss: 481639.6994     \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 9s - loss: 410250.6696     \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 8s - loss: 368132.4292     \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 8s - loss: 365809.8767     \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 7s - loss: 349601.0841     \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 7s - loss: 332854.9230     \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 8s - loss: 329669.9657     \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 7s - loss: 323997.1180     \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 7s - loss: 320699.6154     \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 6s - loss: 316786.1304     \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 6s - loss: 316630.8501     \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 6s - loss: 319375.3738     \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 7s - loss: 311779.3386     \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 7s - loss: 304754.3823     \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 8s - loss: 302084.8784     \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 7s - loss: 299463.0523     \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 7s - loss: 297820.6862     \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 8s - loss: 294598.8515     \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 8s - loss: 292997.9520     \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 8s - loss: 289362.7697     \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 7s - loss: 287014.5624     \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 7s - loss: 286704.8391     \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 7s - loss: 281232.2768     \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 7s - loss: 278575.2771     \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 15s - loss: 277455.5835    \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 13s - loss: 273453.0949    \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 9s - loss: 273162.3374     \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 8s - loss: 270817.5560     \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 11s - loss: 268753.2731    \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 12s - loss: 263121.4586    \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 12s - loss: 261362.2586    \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 6s - loss: 262472.6902     \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 8s - loss: 264572.0276     \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 11s - loss: 264736.9632    \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 13s - loss: 264014.6011    \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 11s - loss: 264159.9297    \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 8s - loss: 264856.4175     \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 6s - loss: 260964.9208     \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 6s - loss: 262598.2514     \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 8s - loss: 259240.6236     \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 7s - loss: 258482.1433     \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 8s - loss: 258868.2134     \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 7s - loss: 258844.9951     \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 5s - loss: 262278.2992     \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 5s - loss: 263623.0864     \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 5s - loss: 261516.5689     \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 6s - loss: 259357.2419     \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 6s - loss: 260178.1520     \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 6s - loss: 259940.3634     \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 6s - loss: 261431.6644     \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 6s - loss: 261147.8479     \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 6s - loss: 260797.3548     \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 6s - loss: 258418.8921     \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 6s - loss: 258887.7359     \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 7s - loss: 260104.7945     \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 6s - loss: 257295.5854     \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 6s - loss: 255882.0542     \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 6s - loss: 255440.5924     \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 6s - loss: 255309.1382     \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 6s - loss: 256440.5486     \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 6s - loss: 257962.4463     \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 6s - loss: 257122.1217     \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 7s - loss: 254185.8382     \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 6s - loss: 252983.8207     \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 6s - loss: 253875.1492     \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 6s - loss: 254447.0350     \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 6s - loss: 252609.9878     \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 6s - loss: 254632.6823     \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 6s - loss: 253966.1876     \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 6s - loss: 255329.7809     \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 6s - loss: 254017.0000     \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 5s - loss: 251805.3419     \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 6s - loss: 251088.5576     \n",
      "Epoch 78/118\n",
      "11399/11399 [==============================] - 6s - loss: 255659.4235     \n",
      "Epoch 79/118\n",
      "11399/11399 [==============================] - 6s - loss: 256518.4779     \n",
      "Epoch 80/118\n",
      "11399/11399 [==============================] - 6s - loss: 254691.5547     \n",
      "Epoch 81/118\n",
      "11399/11399 [==============================] - 7s - loss: 254539.6578     \n",
      "Epoch 82/118\n",
      "11399/11399 [==============================] - 6s - loss: 250864.8443     \n",
      "Epoch 83/118\n",
      "11399/11399 [==============================] - 7s - loss: 249094.3804     \n",
      "Epoch 84/118\n",
      "11399/11399 [==============================] - 6s - loss: 249336.4933     \n",
      "Epoch 85/118\n",
      "11399/11399 [==============================] - 6s - loss: 250196.2740     \n",
      "Epoch 86/118\n",
      "11399/11399 [==============================] - 5s - loss: 249509.2329     \n",
      "Epoch 87/118\n",
      "11399/11399 [==============================] - 6s - loss: 250197.0557     \n",
      "Epoch 88/118\n",
      "11399/11399 [==============================] - 6s - loss: 251080.5270     \n",
      "Epoch 89/118\n",
      "11399/11399 [==============================] - 6s - loss: 250557.8076     \n",
      "Epoch 90/118\n",
      "11399/11399 [==============================] - 6s - loss: 252235.1400     \n",
      "Epoch 91/118\n",
      "11399/11399 [==============================] - 6s - loss: 253639.2265     \n",
      "Epoch 92/118\n",
      "11399/11399 [==============================] - 7s - loss: 252583.4585     \n",
      "Epoch 93/118\n",
      "11399/11399 [==============================] - 10s - loss: 250520.6136    \n",
      "Epoch 94/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399/11399 [==============================] - 8s - loss: 252252.0695     \n",
      "Epoch 95/118\n",
      "11399/11399 [==============================] - 7s - loss: 249591.2852     \n",
      "Epoch 96/118\n",
      "11399/11399 [==============================] - 7s - loss: 255568.4739     \n",
      "Epoch 97/118\n",
      "11399/11399 [==============================] - 7s - loss: 251696.2415     \n",
      "Epoch 98/118\n",
      "11399/11399 [==============================] - 7s - loss: 251487.3969     \n",
      "Epoch 99/118\n",
      "11399/11399 [==============================] - 7s - loss: 249231.7636     \n",
      "Epoch 100/118\n",
      "11399/11399 [==============================] - 8s - loss: 249042.2975     \n",
      "Epoch 101/118\n",
      "11399/11399 [==============================] - 7s - loss: 248302.2286     \n",
      "Epoch 102/118\n",
      "11399/11399 [==============================] - 7s - loss: 246601.3277     \n",
      "Epoch 103/118\n",
      "11399/11399 [==============================] - 7s - loss: 248034.2970     \n",
      "Epoch 104/118\n",
      "11399/11399 [==============================] - 7s - loss: 248524.1815     \n",
      "Epoch 105/118\n",
      "11399/11399 [==============================] - 8s - loss: 248526.7264     \n",
      "Epoch 106/118\n",
      "11399/11399 [==============================] - 6s - loss: 248734.4725     \n",
      "Epoch 107/118\n",
      "11399/11399 [==============================] - 8s - loss: 249131.2053     \n",
      "Epoch 108/118\n",
      "11399/11399 [==============================] - 6s - loss: 249472.9807     \n",
      "Epoch 109/118\n",
      "11399/11399 [==============================] - 6s - loss: 248122.0889     \n",
      "Epoch 110/118\n",
      "11399/11399 [==============================] - 6s - loss: 247538.4777     \n",
      "Epoch 111/118\n",
      "11399/11399 [==============================] - 5s - loss: 246826.0160     \n",
      "Epoch 112/118\n",
      "11399/11399 [==============================] - 6s - loss: 246119.0222     \n",
      "Epoch 113/118\n",
      "11399/11399 [==============================] - 5s - loss: 246176.1124     \n",
      "Epoch 114/118\n",
      "11399/11399 [==============================] - 5s - loss: 247564.8650     \n",
      "Epoch 115/118\n",
      "11399/11399 [==============================] - 5s - loss: 246383.4273     \n",
      "Epoch 116/118\n",
      "11399/11399 [==============================] - 6s - loss: 245388.1248     \n",
      "Epoch 117/118\n",
      "11399/11399 [==============================] - 6s - loss: 246360.6273     \n",
      "Epoch 118/118\n",
      "11399/11399 [==============================] - 6s - loss: 245665.8655     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d4afe1c18>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "instruments = Input(shape=(gene_variants.shape[1],), name = \"instruments\")\n",
    "hidden = [100, 100, 100, 100, 100, 100]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "l2_reg = 0.0001\n",
    "w_reg = regularizers.l2(l2_reg)\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "firstStage = Sequential([\n",
    "    Dense(hidden[0], activation='relu', input_dim = 13980, name='fc1'),\n",
    "    Dense(hidden[1], activation='relu', name = 'fc2'),\n",
    "    Dense(hidden[2], activation='relu', name = 'fc3'),\n",
    "    Dense(hidden[3], activation='relu', name = 'fc4'),\n",
    "    Dense(hidden[4], activation='relu', name = 'fc5'),\n",
    "    Dense(hidden[5], activation='relu', name = 'fc6'),\n",
    "    Dense(2344, activation = 'relu', name = 'output')])\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = 1e-08)\n",
    "    \n",
    "firstStage.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "firstStage.fit(z_train, p_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-6a1387c7290d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2344\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fc1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fc2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fc3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     Dense(1, activation = 'sigmoid', name = 'output')])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "mrna = Input(shape=(expression_levels.shape[1],), name = \"mrna\")\n",
    "hidden = [100, 50]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "l2_reg = 0.0001\n",
    "w_reg = regularizers.l2(l2_reg)\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "p_hat_train = firstStage.predict(z_train)\n",
    "\n",
    "\n",
    "secondStage = Sequential([\n",
    "    Dense(hidden[0], activation='tanh', input_dim = 2344, name='fc1'),\n",
    "    Dense(hidden[1], activation = 'tanh', name = 'fc2'),\n",
    "    Dense(1, activation = 'sigmoid', name = 'output')])\n",
    "    \n",
    "secondStage.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.3153     \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2538     \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2545     \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2564     \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2561     \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2560     \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2560     \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 78/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 79/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 80/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 81/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 82/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 83/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 84/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 85/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 86/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 87/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 88/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 89/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 90/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 91/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 92/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 93/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 94/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 95/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 96/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 97/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 98/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 99/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399/11399 [==============================] - 1s - loss: 0.2559     \n",
      "Epoch 100/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 101/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 102/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 103/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2559     \n",
      "Epoch 104/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n",
      "Epoch 105/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 106/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 107/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2558     \n",
      "Epoch 108/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 109/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 110/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2556     \n",
      "Epoch 111/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2558     \n",
      "Epoch 112/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n",
      "Epoch 113/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n",
      "Epoch 114/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n",
      "Epoch 115/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.2557     \n",
      "Epoch 116/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n",
      "Epoch 117/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2556     \n",
      "Epoch 118/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.2557     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d88f71d30>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondStage.fit(p_hat_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.1257     \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0226     \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0112     \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0080     \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0059     \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0051     \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0043     \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0037     \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0033     \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0027     \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0021     \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0016     \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0016     \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 1s - loss: 9.6668e-04     \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 1s - loss: 8.6720e-04     \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 1s - loss: 7.2330e-04     \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 1s - loss: 8.1185e-04     \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 1s - loss: 6.5017e-04     \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 1s - loss: 6.1325e-04     \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 1s - loss: 4.8838e-04     \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 1s - loss: 9.2641e-04     \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 1s - loss: 4.9983e-04     \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 1s - loss: 5.7343e-04     \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.9778e-04     \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.7634e-04     \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.5283e-04     \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.3075e-04     \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.1255e-04     \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.9634e-04     \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.8182e-04     \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.6810e-04     \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.5625e-04     \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.4545e-04     \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.3540e-04     \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.2679e-04     \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.1936e-04     \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.1080e-04     \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.0346e-04     \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.6701e-05     \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.0475e-05     \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.4651e-05     \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.9287e-05     \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.4309e-05     \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.9683e-05     \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.5332e-05     \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.1316e-05     \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.7586e-05     \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.4101e-05     \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.0827e-05     \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.7773e-05     \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.4934e-05     \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.2266e-05     \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.9763e-05     \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.1235e-05     \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.6726e-05     \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.3283e-05     \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.1276e-05     \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.9445e-05     \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.7731e-05     \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.6123e-05     \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.4602e-05     \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.3184e-05     \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.1856e-05     \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.0598e-05     \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.9426e-05     \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.8326e-05     \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.7273e-05     \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.6291e-05     \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.5365e-05     \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.4493e-05     \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.3675e-05     \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.2901e-05     \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.2176e-05     \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.1489e-05     \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.0843e-05     \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.0234e-05     \n",
      "Epoch 78/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.6597e-06     \n",
      "Epoch 79/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.1169e-06     \n",
      "Epoch 80/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.6062e-06     \n",
      "Epoch 81/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.1259e-06     \n",
      "Epoch 82/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.6718e-06     \n",
      "Epoch 83/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.2426e-06     \n",
      "Epoch 84/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.8385e-06     \n",
      "Epoch 85/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.4576e-06     \n",
      "Epoch 86/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.0971e-06     \n",
      "Epoch 87/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.7580e-06     \n",
      "Epoch 88/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.4370e-06     \n",
      "Epoch 89/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.1340e-06     \n",
      "Epoch 90/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.8487e-06     \n",
      "Epoch 91/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.5785e-06     \n",
      "Epoch 92/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.3247e-06     \n",
      "Epoch 93/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.0838e-06     \n",
      "Epoch 94/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.8576e-06     \n",
      "Epoch 95/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399/11399 [==============================] - 0s - loss: 3.6432e-06     \n",
      "Epoch 96/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.4412e-06     \n",
      "Epoch 97/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.2500e-06     \n",
      "Epoch 98/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.0702e-06     \n",
      "Epoch 99/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.8995e-06     \n",
      "Epoch 100/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.7388e-06     \n",
      "Epoch 101/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.5875e-06     \n",
      "Epoch 102/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.4441e-06     \n",
      "Epoch 103/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.3085e-06     \n",
      "Epoch 104/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.1811e-06     \n",
      "Epoch 105/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.0601e-06     \n",
      "Epoch 106/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.9458e-06     \n",
      "Epoch 107/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.8385e-06     \n",
      "Epoch 108/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.7363e-06     \n",
      "Epoch 109/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.6401e-06     \n",
      "Epoch 110/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.5497e-06     \n",
      "Epoch 111/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.4647e-06     \n",
      "Epoch 112/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.3824e-06     \n",
      "Epoch 113/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.3071e-06     \n",
      "Epoch 114/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.2339e-06     \n",
      "Epoch 115/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.1660e-06     \n",
      "Epoch 116/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.1021e-06     \n",
      "Epoch 117/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.0411e-06     \n",
      "Epoch 118/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.8304e-07     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d88b15fd0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondStage.fit(p_train, y_train, epochs = epochs, batch_size = batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "p_hat_test = firstStage.predict(z_test)\n",
    "y_hat_test_deepiv = secondStage.predict(p_hat_test)\n",
    "y_hat_test_naive = secondStage.predict(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_deepiv = (y_hat_test_deepiv > .5).astype(int)\n",
    "y_hat_test_naive = (y_hat_test_naive > .5).astype(int)\n",
    "y_hat_test_naive = y_hat_test_naive.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267, 1)\n",
      "(1267, 1)\n",
      "(1267, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_hat_test_deepiv.shape)\n",
    "print(y_hat_test_naive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test[:,np.newaxis]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error_vec_deepiv = (y_test != y_hat_test_deepiv).astype(int)\n",
    "class_error_vec_naive= (y_test != y_hat_test_naive).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepIV classification error is 0.0718232044198895\n",
      "OneStage classification error is 0.0055248618784530384\n"
     ]
    }
   ],
   "source": [
    "class_error_deepiv = np.mean(class_error_vec_deepiv)\n",
    "class_error_naive = np.mean(class_error_vec_naive)\n",
    "print('DeepIV classification error is', class_error_deepiv)\n",
    "print('OneStage classification error is', class_error_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
