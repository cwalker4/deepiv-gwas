{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train first-stage model\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from model.utils import Params\n",
    "from model.utils import set_logger\n",
    "from model.data_loader import load_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('stage', help=\"Which stage network to train\")\n",
    "parser.add_argument('--model', default=\"base_model\", help=\"Name of directory containing params.json\")\n",
    "parser.add_argument('--restore_dir', default=None,\n",
    "                    help=\"Optional, directory containing weights to reload before training\")\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics,  params, model_dir, restore_file=None):\n",
    "    '''\n",
    "    Trains the Keras model\n",
    "\n",
    "    Args:\n",
    "        model: (keras.Model) the neural network\n",
    "        train_data: (dict) train data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validation data with keys 'data' and 'labels'\n",
    "        optimizer: (keras.optimizers) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_labels and batch_output and computes batch loss\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using output and labels of a batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log \n",
    "    '''\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    check_path = \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpointer = ModelCheckpoint(model_dir, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "    model.fit(train_data['data'], train_data['labels'], epochs=params.epochs,\n",
    "              batch_size = params.batch_size, validation_data=(val_data['data'], val_data['labels']),\n",
    "              callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the random seed for the whole graph for reproductible experiments\n",
    "    np.random.seed(123)\n",
    "    tf.set_random_seed(123)\n",
    "\n",
    "    # Load the parameters from the experiment params.json file in model_dir\n",
    "    args = parser.parse_args()\n",
    "    json_path = os.path.join('experiments', args.stage, args.model, 'params.json')\n",
    "    assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
    "    params = Params(json_path)\n",
    "\n",
    "    # Check that we are not overwriting some previous experiment\n",
    "    # Comment these lines if you are developing your model and don't care about overwritting\n",
    "    #model_dir_has_best_weights = os.path.isdir(os.path.join(args.model_dir, \"best_weights\"))\n",
    "    #overwritting = model_dir_has_best_weights and args.restore_dir is None\n",
    "    #assert not overwritting, \"Weights found in model_dir, aborting to avoid overwrite\"\n",
    "\n",
    "    # Set the logger\n",
    "    set_logger(os.path.join('experiments', args.stage, args.model, 'train.log'))\n",
    "\n",
    "    logging.info(\"Loading the datasets...\")\n",
    "\n",
    "    # load data\n",
    "    data_dir = os.path.join('data', args.stage)\n",
    "    data = load_data(['train', 'val'], args.stage, 'data')\n",
    "    train_data = data['train']\n",
    "    val_data = data['val']\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "\n",
    "    # specifying train and val dataset sizes\n",
    "    params.train_size = train_data['size']\n",
    "    params.val_size = val_data['size']\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "\n",
    "    # define the model \n",
    "    input_layer = Input(shape=(train_data['data'].shape[1],))\n",
    "    output_layer = Dense(train_data['labels'].shape[1], activation=params.output_activation)\n",
    "\n",
    "    model = net.feed_forward_net(input_layer, output_layer, params)\n",
    "    optimizer = optimizers.adam(lr=params.learning_rate) # add others params to .json if we want\n",
    "\n",
    "    # fetch loss function and metrics\n",
    "    metrics = net.metrics\n",
    "\n",
    "    # train the model\n",
    "    logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
    "    train_and_evaluate(model, train_data, val_data, optimizer, metrics,\n",
    "                       params, args.model_dir) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
