{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "from deepiv.models import Treatment, Response\n",
    "import deepiv.architectures as architectures\n",
    "import deepiv.densities as densities\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_levels_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/expression_levels_subset.csv')\n",
    "gene_variants_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/gene_variants.csv')\n",
    "outcomes_raw = pd.read_csv('/Users/billyf/deepiv-gwas/derived_data/outcomes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim(z) =  (13980, 12667)\n",
      "Dim(p) =  (2344, 12667)\n",
      "Dim(y) =  (12666, 2)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "assert(gene_variants_raw.shape[1] - 1 == outcomes_raw.shape[0])\n",
    "print(\"Dim(z) = \", gene_variants_raw.shape)\n",
    "print(\"Dim(p) = \", expression_levels_raw.shape)\n",
    "print(\"Dim(y) = \", outcomes_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_levels = expression_levels_raw.drop('mrna', axis = 1).transpose().as_matrix()\n",
    "gene_variants = gene_variants_raw.drop('hugo', axis = 1).transpose().as_matrix()\n",
    "outcomes = outcomes_raw['outcome'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim(z) =  (12666, 13980)\n",
      "Dim(p) =  (12666, 2344)\n",
      "Dim(y) =  (12666,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dim(z) = \", gene_variants.shape)\n",
    "print(\"Dim(p) = \", expression_levels.shape)\n",
    "print(\"Dim(y) = \", outcomes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train, z_test, p_train, p_test, y_train, y_test = train_test_split(gene_variants, expression_levels, outcomes, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training first stage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting treatment\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1156: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/billyf/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/118\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100, 2344) for Tensor 'concatenate_1_target:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-34c8f427c076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m treatment_model.fit(gene_variants, expression_levels,\n\u001b[1;32m     25\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                    batch_size = batch_size)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1105\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100, 2344) for Tensor 'concatenate_1_target:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "instruments = Input(shape=(gene_variants.shape[1],), name = \"instruments\")\n",
    "hidden = [7000, 4000, 3000]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "est_treat = architectures.feed_forward_net(instruments,\n",
    "                                          lambda x: densities.mixture_of_gaussian_output(x, n_components),\n",
    "                                          hidden_layers = hidden,\n",
    "                                          activations = activation)\n",
    "\n",
    "treatment_model = Treatment(inputs=[instruments], outputs=est_treat)\n",
    "print(\"Fitting treatment\")\n",
    "treatment_model.compile('adam',\n",
    "                       loss='mixture_of_gaussians',\n",
    "                       n_components = n_components)\n",
    "\n",
    "treatment_model.fit(gene_variants, expression_levels,\n",
    "                   epochs = epochs,\n",
    "                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 12s - loss: 1339117.8654    \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 11s - loss: 1291711.7836    \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 11s - loss: 1185221.5010    \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 12s - loss: 741653.2536    \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 12s - loss: 532855.0658    \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 12s - loss: 433080.4761    \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 11s - loss: 373094.1020    \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 11s - loss: 352683.6638    \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 12s - loss: 340079.8035    \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 12s - loss: 329271.9627    \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 13s - loss: 322825.1349    \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 15s - loss: 326628.3609    \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 14s - loss: 326269.1734    \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 13s - loss: 320263.4252    \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 12s - loss: 308281.1211    \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 12s - loss: 304748.5051    \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 12s - loss: 300946.0813    \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 13s - loss: 291360.5723    \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 14s - loss: 281013.9669    \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 14s - loss: 270448.5993    \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 15s - loss: 269607.0301    \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 14s - loss: 272887.4047    \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 12s - loss: 274051.7845    \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 13s - loss: 271491.1003    \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 12s - loss: 272151.3247    \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 12s - loss: 266979.9220    \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 12s - loss: 262516.9568    \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 12s - loss: 262020.5875    \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 12s - loss: 262867.8025    \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 12s - loss: 264341.2038    \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 12s - loss: 266116.9446    \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 12s - loss: 268316.6142    \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 12s - loss: 265055.7940    \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 12s - loss: 262327.4335    \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 12s - loss: 264476.7775    \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 12s - loss: 261939.8633    \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 12s - loss: 259527.9836    \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 12s - loss: 258254.5323    \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 12s - loss: 257319.2048    \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 12s - loss: 257351.6978    \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 12s - loss: 259886.8085    \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 12s - loss: 259269.2035    \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 12s - loss: 261087.1675    \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 12s - loss: 262827.1658    \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 12s - loss: 263466.5461    \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 12s - loss: 256548.5004    \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 12s - loss: 253032.8248    \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 12s - loss: 254883.3136    \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 12s - loss: 255060.4935    \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 12s - loss: 255092.8582    \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 13s - loss: 255033.7183    \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 12s - loss: 255781.0429    \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 15s - loss: 257206.4217    \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 15s - loss: 257093.7341    \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 16s - loss: 254456.0242    \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 16s - loss: 254384.9937    \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 16s - loss: 254176.8580    \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 13s - loss: 253170.0688    \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 14s - loss: 251638.9311    \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 14s - loss: 253862.8471    \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 14s - loss: 251629.6003    \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 12s - loss: 251401.4638    \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 16s - loss: 253535.2012    \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 15s - loss: 256981.2276    \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 14s - loss: 254632.6938    \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 14s - loss: 249971.3523    \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 14s - loss: 248476.8488    \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 14s - loss: 249816.6207    \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 12s - loss: 249686.9058    \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 12s - loss: 250364.5684    \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 14s - loss: 248722.0976    \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 13s - loss: 251225.0083    \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 13s - loss: 249362.6865    \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 12s - loss: 248333.0336    \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 12s - loss: 245623.3035    \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 13s - loss: 245831.4804    \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 14s - loss: 247435.6189    \n",
      "Epoch 78/118\n",
      "11399/11399 [==============================] - 15s - loss: 248783.6416    \n",
      "Epoch 79/118\n",
      "11399/11399 [==============================] - 11s - loss: 247458.5282    \n",
      "Epoch 80/118\n",
      "11399/11399 [==============================] - 11s - loss: 246929.6201    \n",
      "Epoch 81/118\n",
      "11399/11399 [==============================] - 11s - loss: 247747.8180    \n",
      "Epoch 82/118\n",
      "11399/11399 [==============================] - 11s - loss: 251094.2481    \n",
      "Epoch 83/118\n",
      "11399/11399 [==============================] - 11s - loss: 250545.6964    \n",
      "Epoch 84/118\n",
      "11399/11399 [==============================] - 11s - loss: 252697.2691    \n",
      "Epoch 85/118\n",
      "11399/11399 [==============================] - 11s - loss: 249391.4258    \n",
      "Epoch 86/118\n",
      "11399/11399 [==============================] - 11s - loss: 248724.4540    \n",
      "Epoch 87/118\n",
      "11399/11399 [==============================] - 11s - loss: 246492.6824    \n",
      "Epoch 88/118\n",
      "11399/11399 [==============================] - 11s - loss: 247530.0764    \n",
      "Epoch 89/118\n",
      "11399/11399 [==============================] - 11s - loss: 247565.3592    \n",
      "Epoch 90/118\n",
      "11399/11399 [==============================] - 12s - loss: 247666.6483    \n",
      "Epoch 91/118\n",
      "11399/11399 [==============================] - 11s - loss: 245450.1759    \n",
      "Epoch 92/118\n",
      "11399/11399 [==============================] - 11s - loss: 246914.3315    \n",
      "Epoch 93/118\n",
      "11399/11399 [==============================] - 11s - loss: 246895.7976    \n",
      "Epoch 94/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399/11399 [==============================] - 12s - loss: 244794.7711    \n",
      "Epoch 95/118\n",
      "11399/11399 [==============================] - 10s - loss: 243514.6421    \n",
      "Epoch 96/118\n",
      "11399/11399 [==============================] - 10s - loss: 243682.7021    \n",
      "Epoch 97/118\n",
      "11399/11399 [==============================] - 11s - loss: 245652.1927    \n",
      "Epoch 98/118\n",
      "11399/11399 [==============================] - 12s - loss: 244905.2011    \n",
      "Epoch 99/118\n",
      "11399/11399 [==============================] - 12s - loss: 243624.7191    \n",
      "Epoch 100/118\n",
      "11399/11399 [==============================] - 12s - loss: 242562.8387    \n",
      "Epoch 101/118\n",
      "11399/11399 [==============================] - 12s - loss: 242312.4937    \n",
      "Epoch 102/118\n",
      "11399/11399 [==============================] - 11s - loss: 243826.1177    \n",
      "Epoch 103/118\n",
      "11399/11399 [==============================] - 11s - loss: 243601.1154    \n",
      "Epoch 104/118\n",
      "11399/11399 [==============================] - 11s - loss: 243365.3466    \n",
      "Epoch 105/118\n",
      "11399/11399 [==============================] - 11s - loss: 260365.7758    \n",
      "Epoch 106/118\n",
      "11399/11399 [==============================] - 11s - loss: 247487.9098    \n",
      "Epoch 107/118\n",
      "11399/11399 [==============================] - 11s - loss: 244661.9074    \n",
      "Epoch 108/118\n",
      "11399/11399 [==============================] - 11s - loss: 242916.0361    \n",
      "Epoch 109/118\n",
      "11399/11399 [==============================] - 12s - loss: 243497.9272    \n",
      "Epoch 110/118\n",
      "11399/11399 [==============================] - 13s - loss: 243816.7551    \n",
      "Epoch 111/118\n",
      "11399/11399 [==============================] - 11s - loss: 244135.6653    \n",
      "Epoch 112/118\n",
      "11399/11399 [==============================] - 11s - loss: 242869.2821    \n",
      "Epoch 113/118\n",
      "11399/11399 [==============================] - 11s - loss: 242674.8391    \n",
      "Epoch 114/118\n",
      "11399/11399 [==============================] - 10s - loss: 241698.8577    \n",
      "Epoch 115/118\n",
      "11399/11399 [==============================] - 12s - loss: 243023.4791    \n",
      "Epoch 116/118\n",
      "11399/11399 [==============================] - 12s - loss: 243727.6243    \n",
      "Epoch 117/118\n",
      "11399/11399 [==============================] - 11s - loss: 242085.4142    \n",
      "Epoch 118/118\n",
      "11399/11399 [==============================] - 11s - loss: 242277.5401    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d41b92978>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "instruments = Input(shape=(gene_variants.shape[1],), name = \"instruments\")\n",
    "hidden = [200, 200, 200, 200, 200]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "l2_reg = 0.0001\n",
    "w_reg = regularizers.l2(l2_reg)\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "firstStage = Sequential([\n",
    "    Dense(hidden[0], activation='relu', input_dim = 13980, name='fc1'),\n",
    "    Dense(hidden[1], activation='relu', name = 'fc2'),\n",
    "    Dense(hidden[2], activation='relu', name = 'fc3'),\n",
    "    Dense(hidden[3], activation='relu', name = 'fc4'),\n",
    "    Dense(hidden[4], activation='relu', name = 'fc5'),\n",
    "    Dense(2344, activation = 'relu', name = 'output')])\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon = 1e-08)\n",
    "    \n",
    "firstStage.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "firstStage.fit(z_train, p_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First stage of DeepIV MSE is [[1.06708900e+02 1.50327415e+01 8.89136827e+03 ... 1.26000400e+03\n",
      "  3.08396764e+06 5.16652900e+00]\n",
      " [1.14490000e+02 2.49472519e+03 1.41933623e+04 ... 1.20905487e+03\n",
      "  5.54086210e+04 2.58888100e+02]\n",
      " [8.31168900e+02 1.26025000e+03 3.61781587e+03 ... 1.19562595e+03\n",
      "  1.34497661e+03 3.46096890e+01]\n",
      " ...\n",
      " [1.43041600e+02 2.03040360e+03 9.09009982e+04 ... 8.80031610e+03\n",
      "  1.73722240e+05 8.87444100e+02]\n",
      " [1.28595600e+02 2.41797878e+02 2.17133832e+04 ... 2.84728960e+03\n",
      "  4.54513644e+05 1.55500900e+02]\n",
      " [3.03050250e+01 1.34794930e+03 3.32006735e+01 ... 6.42115600e+02\n",
      "  4.24360000e+04 1.05560010e+03]]\n"
     ]
    }
   ],
   "source": [
    "p_hat_test = firstStage.predict(z_test)\n",
    "\n",
    "p_residuals = p_test - p_hat_test\n",
    "p_mse = p_residuals * p_residuals\n",
    "print(\"First stage of DeepIV MSE is\", p_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(outcomes)\n",
    "epochs = int(1500000./float(n)) # heuristic to get epochs\n",
    "batch_size = 100\n",
    "\n",
    "mrna = Input(shape=(expression_levels.shape[1],), name = \"mrna\")\n",
    "hidden = [100, 50]\n",
    "\n",
    "\n",
    "activation = \"relu\"\n",
    "l2_reg = 0.0001\n",
    "w_reg = regularizers.l2(l2_reg)\n",
    "\n",
    "n_components = expression_levels.shape[1]\n",
    "\n",
    "p_hat_train = firstStage.predict(z_train)\n",
    "\n",
    "\n",
    "secondStage = Sequential([\n",
    "    Dense(hidden[0], activation='tanh', input_dim = 2344, name='fc1'),\n",
    "    Dense(hidden[1], activation = 'tanh', name = 'fc2'),\n",
    "    Dense(1, activation = 'sigmoid', name = 'output')])\n",
    "    \n",
    "secondStage.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0796     \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0314     \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0306     \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0255     \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0240     \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0274     \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0282     \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0269     \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0263     \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0294     \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0294     \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0432     \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0319     \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0379     \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0293     \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0331     \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0327     \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0283     \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0311     \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0334     \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0675     \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0589     \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0623     \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0636     \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0619     \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0490     \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0548     \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0579     \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0571     \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0558     \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0498     \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0318     \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0313     \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0355     \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0586     \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0572     \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0559     \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0578     \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0565     \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0441     \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0550     \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0343     \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0300     \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0283     \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0280     \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0253     \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0264     \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0289     \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0278     \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0280     \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0314     \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0273     \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0289     \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0323     \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0299     \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0290     \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0284     \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0281     \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0284     \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0282     \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0288     \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0400     \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0532     \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0465     \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0347     \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0339     \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0323     \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0528     \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0448     \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0438     \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0311     \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0241     \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0248     \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0242     \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0276     \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0270     \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0274     \n",
      "Epoch 78/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0273     \n",
      "Epoch 79/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0267     \n",
      "Epoch 80/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0256     \n",
      "Epoch 81/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0248     \n",
      "Epoch 82/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0268     \n",
      "Epoch 83/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0263     \n",
      "Epoch 84/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0263     \n",
      "Epoch 85/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0253     \n",
      "Epoch 86/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0243     \n",
      "Epoch 87/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0229     \n",
      "Epoch 88/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0231     \n",
      "Epoch 89/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0308     \n",
      "Epoch 90/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0287     \n",
      "Epoch 91/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0398     \n",
      "Epoch 92/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0311     \n",
      "Epoch 93/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0291     \n",
      "Epoch 94/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0278     \n",
      "Epoch 95/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0286     \n",
      "Epoch 96/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0282     \n",
      "Epoch 97/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0282     \n",
      "Epoch 98/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0275     \n",
      "Epoch 99/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399/11399 [==============================] - 0s - loss: 0.0257     \n",
      "Epoch 100/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0252     \n",
      "Epoch 101/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0266     \n",
      "Epoch 102/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0242     \n",
      "Epoch 103/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0278     \n",
      "Epoch 104/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0263     \n",
      "Epoch 105/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0236     \n",
      "Epoch 106/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0243     \n",
      "Epoch 107/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0249     \n",
      "Epoch 108/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0231     \n",
      "Epoch 109/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0244     \n",
      "Epoch 110/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0267     \n",
      "Epoch 111/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0266     \n",
      "Epoch 112/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0264     \n",
      "Epoch 113/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0260     \n",
      "Epoch 114/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0258     \n",
      "Epoch 115/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0253     \n",
      "Epoch 116/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0251     \n",
      "Epoch 117/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0255     \n",
      "Epoch 118/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0244     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6993ab00>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondStage.fit(p_hat_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "oneStage = Sequential([\n",
    "    Dense(hidden[0], activation='tanh', input_dim = 2344, name='fc1'),\n",
    "    Dense(hidden[1], activation = 'tanh', name = 'fc2'),\n",
    "    Dense(1, activation = 'sigmoid', name = 'output')])\n",
    "    \n",
    "oneStage.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0679     \n",
      "Epoch 2/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0059     \n",
      "Epoch 3/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0033     \n",
      "Epoch 4/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0027     \n",
      "Epoch 5/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0022     \n",
      "Epoch 6/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0016     \n",
      "Epoch 7/118\n",
      "11399/11399 [==============================] - 1s - loss: 0.0014     \n",
      "Epoch 8/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0020     \n",
      "Epoch 9/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0011     \n",
      "Epoch 10/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0013     \n",
      "Epoch 11/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0011     \n",
      "Epoch 12/118\n",
      "11399/11399 [==============================] - 0s - loss: 0.0010       \n",
      "Epoch 13/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.5115e-04     \n",
      "Epoch 14/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.5465e-04     \n",
      "Epoch 15/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.8578e-04     \n",
      "Epoch 16/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.7383e-04     \n",
      "Epoch 17/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.7496e-04     \n",
      "Epoch 18/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.4976e-04     \n",
      "Epoch 19/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.2780e-05     \n",
      "Epoch 20/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.8246e-05     \n",
      "Epoch 21/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.9275e-05     \n",
      "Epoch 22/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.7280e-05     \n",
      "Epoch 23/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.6493e-05     \n",
      "Epoch 24/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.0484e-05     \n",
      "Epoch 25/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.5261e-05     \n",
      "Epoch 26/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.0434e-05     \n",
      "Epoch 27/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.4457e-05     \n",
      "Epoch 28/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.5768e-05     \n",
      "Epoch 29/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.9297e-05     \n",
      "Epoch 30/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.6460e-05     \n",
      "Epoch 31/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.4216e-05     \n",
      "Epoch 32/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.0937e-05     \n",
      "Epoch 33/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.7120e-05     \n",
      "Epoch 34/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.5482e-05     \n",
      "Epoch 35/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.4206e-05     \n",
      "Epoch 36/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.3314e-05     \n",
      "Epoch 37/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.2493e-05     \n",
      "Epoch 38/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.1637e-05     \n",
      "Epoch 39/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.0962e-05     \n",
      "Epoch 40/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.0391e-05     \n",
      "Epoch 41/118\n",
      "11399/11399 [==============================] - 1s - loss: 9.6761e-06     \n",
      "Epoch 42/118\n",
      "11399/11399 [==============================] - 0s - loss: 9.0670e-06     \n",
      "Epoch 43/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.5132e-06     \n",
      "Epoch 44/118\n",
      "11399/11399 [==============================] - 0s - loss: 8.0045e-06     \n",
      "Epoch 45/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.5066e-06     \n",
      "Epoch 46/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.2293e-06     \n",
      "Epoch 47/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.6714e-05     \n",
      "Epoch 48/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.4628e-05     \n",
      "Epoch 49/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.6993e-05     \n",
      "Epoch 50/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.1437e-05     \n",
      "Epoch 51/118\n",
      "11399/11399 [==============================] - 0s - loss: 7.1645e-06     \n",
      "Epoch 52/118\n",
      "11399/11399 [==============================] - 0s - loss: 6.2460e-06     \n",
      "Epoch 53/118\n",
      "11399/11399 [==============================] - 0s - loss: 5.2814e-06     \n",
      "Epoch 54/118\n",
      "11399/11399 [==============================] - 0s - loss: 4.9226e-06     \n",
      "Epoch 55/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.3319e-06     \n",
      "Epoch 56/118\n",
      "11399/11399 [==============================] - 0s - loss: 3.0870e-06     \n",
      "Epoch 57/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.9034e-06     \n",
      "Epoch 58/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.7377e-06     \n",
      "Epoch 59/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.5956e-06     \n",
      "Epoch 60/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.4529e-06     \n",
      "Epoch 61/118\n",
      "11399/11399 [==============================] - 0s - loss: 2.3224e-06     \n",
      "Epoch 62/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.1933e-06     \n",
      "Epoch 63/118\n",
      "11399/11399 [==============================] - 1s - loss: 2.0830e-06     \n",
      "Epoch 64/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.9861e-06     \n",
      "Epoch 65/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.8755e-06     \n",
      "Epoch 66/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.7782e-06     \n",
      "Epoch 67/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.6819e-06     \n",
      "Epoch 68/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.5839e-06     \n",
      "Epoch 69/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.5133e-06     \n",
      "Epoch 70/118\n",
      "11399/11399 [==============================] - 0s - loss: 1.4221e-06     \n",
      "Epoch 71/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.3522e-06     \n",
      "Epoch 72/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.2877e-06     \n",
      "Epoch 73/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.3219e-06     \n",
      "Epoch 74/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.2181e-06     \n",
      "Epoch 75/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.1479e-06     \n",
      "Epoch 76/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.0767e-06     \n",
      "Epoch 77/118\n",
      "11399/11399 [==============================] - 1s - loss: 1.0203e-06     \n",
      "Epoch 78/118\n",
      " 9500/11399 [========================>.....] - ETA: 0s - loss: 1.0111e-06"
     ]
    }
   ],
   "source": [
    "oneStage.fit(p_train, y_train, epochs = epochs, batch_size = batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "p_hat_test = firstStage.predict(z_test)\n",
    "y_hat_test_deepiv = secondStage.predict(p_hat_test)\n",
    "y_hat_test_naive = oneStage.predict(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_deepiv = (y_hat_test_deepiv > .5).astype(int)\n",
    "y_hat_test_naive = (y_hat_test_naive > .5).astype(int)\n",
    "y_hat_test_naive = y_hat_test_naive.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267, 1)\n",
      "(1267, 1)\n",
      "(1267, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_hat_test_deepiv.shape)\n",
    "print(y_hat_test_naive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 1)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test[:,np.newaxis]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error_vec_deepiv = (y_test != y_hat_test_deepiv).astype(int)\n",
    "class_error_vec_naive= (y_test != y_hat_test_naive).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepIV classification error is 0.03314917127071823\n",
      "OneStage classification error is 0.0\n"
     ]
    }
   ],
   "source": [
    "class_error_deepiv = np.mean(class_error_vec_deepiv)\n",
    "class_error_naive = np.mean(class_error_vec_naive)\n",
    "print('DeepIV classification error is', class_error_deepiv)\n",
    "print('OneStage classification error is', class_error_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
